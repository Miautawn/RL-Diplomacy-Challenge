{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511eb14e-b585-4a46-bc12-250d2a348541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import uniform_, xavier_uniform_, zeros_\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from utilities.utility_functions import decompress_dict\n",
    "from settings import MODEL_DATA_PATHS, DATA_BLUEPRINT, DATA_FEATURES, H_PARAMETERS\n",
    "from supervised_model import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7f9ea3-d9b1-4545-bb91-352bd9bd282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8229b48-90b0-44b4-a9c5-5db568d1c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a67a30-6fd6-4081-8c3b-26506e9fbfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyDataset(IterableDataset):\n",
    "    def __init__(self, file_path: str, shuffle: bool, shuffle_buffer_size: int) -> None:\n",
    "        super(DiplomacyDataset).__init__()\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_buffer_size = shuffle_buffer_size\n",
    "        \n",
    "        if self.shuffle and self.shuffle_buffer_size <= 1:\n",
    "            raise ValueError(\"Bad shuffle buffer size.\"\\\n",
    "                            \"If you want to shuffle the iterable data, you have to define a positive > 1 shuffle buffer size.\"\\\n",
    "                            f\"\\nGot 'shuffle': {self.shuffle}, shuffle_buffer_size: {self.shuffle_buffer_size}\")\n",
    "        \n",
    "    def sequential_iterator(self):\n",
    "        # return iter(range(100))\n",
    "        return open(self.file_path, \"r\")\n",
    "        \n",
    "    def shuffle_iterator(self):\n",
    "        shuffle_buffer = []\n",
    "        local_iterator = self.sequential_iterator()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # fill up the initial buffer\n",
    "            for _ in range(self.shuffle_buffer_size):\n",
    "                shuffle_buffer.append(next(local_iterator))\n",
    "        except StopIteration:\n",
    "            # in the scenario of either too small dataset file or too big buffer size,\n",
    "            # shrink the shuffle buffer\n",
    "\n",
    "            LOGGER.info(\"Either the dataset file is too small, or shuffle buffer size is too big for the file. \"\\\n",
    "                        \"Shrinking the buffer...\")\n",
    "            self.shuffle_buffer_size = len(shuffle_buffer)\n",
    "            \n",
    "        # main loop\n",
    "        while True:\n",
    "            remove_index = random.randint(0, len(shuffle_buffer) - 1)\n",
    "            yield shuffle_buffer.pop(remove_index)\n",
    "            \n",
    "            try:\n",
    "                shuffle_buffer.append(next(local_iterator))\n",
    "            except StopIteration:\n",
    "                break\n",
    "                \n",
    "        while len(shuffle_buffer) > 0:\n",
    "            remove_index = random.randint(0, len(shuffle_buffer) - 1)\n",
    "            yield shuffle_buffer.pop(remove_index)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if(self.shuffle):\n",
    "            self.iterator = self.shuffle_iterator()\n",
    "        else:\n",
    "            self.iterator = self.sequential_iterator()\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        return decompress_dict( next(self.iterator) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f10c2af-7011-447c-b677-89fe6d820c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = H_PARAMETERS[\"batch_size\"]\n",
    "shuffle = False\n",
    "shuffle_buffer_size = 1\n",
    "file_path = \"data/model_data/full_dataset_training.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef11e484-f44d-409d-b156-a9551214f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DiplomacyDataset(file_path = file_path, shuffle = shuffle, shuffle_buffer_size = shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07aa8a4e-ef8b-4803-8995-b583e267a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLE_LEN_FEATURES = [key for key, value in DATA_BLUEPRINT.items() if value[\"shape\"] == [None]]\n",
    "\n",
    "def custom_collate_fn(batch: dict, pad_batch: bool = False):\n",
    "    \"\"\"\n",
    "    Modified default pytorch collate function.\n",
    "    It is meant to collate the received dictionary values onto original keys,\n",
    "    also pad the non-fixed dimension features to max length of their group per batch.\n",
    "    \n",
    "    If you modify the project, please pay attention to this function,\n",
    "    as it single-purpose and should be extented.\n",
    "    \"\"\"\n",
    "    element = batch[0]\n",
    "    \n",
    "    # scalars\n",
    "    if isinstance(element, float):\n",
    "        return torch.tensor(batch, dtype=torch.float32)\n",
    "    elif isinstance(element, int):\n",
    "        return torch.tensor(batch, dtype=torch.int32)\n",
    "    elif isinstance(element, str):\n",
    "        return batch\n",
    "    \n",
    "    # lists NOTE: WORKS WITH ONLY 1-D lists!!!\n",
    "    elif isinstance(element, list):\n",
    "        first_list_element = element[0]\n",
    "        \n",
    "        # call collate_fn recursevly to get an the type of list elements\n",
    "        list_type = custom_collate_fn(element).dtype\n",
    "            \n",
    "        # pad the lists if needed\n",
    "        if pad_batch:\n",
    "            max_size = max([len(element) for element in batch])\n",
    "            batch = [element + [0]*(max_size - len(element)) for element in batch]\n",
    "        \n",
    "        return torch.tensor(batch, dtype = list_type)\n",
    "    \n",
    "    # call collate_fn recursevely to collate all values of the keys amongst the batch of dictionaries\n",
    "    elif isinstance(element, dict):\n",
    "        return dict(\n",
    "            {\n",
    "                key: custom_collate_fn(\n",
    "                    batch = [dictionary[key] for dictionary in batch],\n",
    "                    pad_batch = key in VARIABLE_LEN_FEATURES\n",
    "                ) for key in element\n",
    "            }\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a18a81-b689-4857-8ea6-cbeb5308ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size = batch_size, collate_fn = custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75382c7-0972-42b3-aaff-dde140491442",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc78ece-3050-474b-980d-51dd3c2d1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.layers.dropout import SeededDropoutWrapper\n",
    "from utilities.layers.attention import StaticAttentionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7e13f6-2fa7-4e29-9372-a2caa31aca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,data_features: dict, h_params: dict):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.h_params = h_params\n",
    "        self.data_features = data_features\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(\n",
    "            input_size = self.h_params[\"order_emb_size\"] + self.h_params[\"attn_size\"],\n",
    "            hidden_size = self.h_params['lstm_size']\n",
    "        )\n",
    "        \n",
    "        self.decoder_cell = SeededDropoutWrapper(\n",
    "            cell = self.lstm_cell,\n",
    "            n_cell_states = 2, # LSTM has 2 states\n",
    "            variational_recurrent = self.h_params[\"use_variational_recurrent_dropout\"],\n",
    "        )\n",
    "        \n",
    "        self.another_decoder_cell = StaticAttentionWrapper(\n",
    "            cell=self.decoder_cell,\n",
    "            attention_memory_size = self.h_params[\"attn_size\"],\n",
    "            attention_memory_time = self.data_features[\"n_nodes\"],\n",
    "            batch_size = self.h_params[\"batch_size\"],\n",
    "            output_attention=False\n",
    "        )\n",
    "        \n",
    "#         decoder_cell = StaticAttentionWrapper(cell=decoder_cell,\n",
    "#                                       memory=board_state_conv,\n",
    "#                                       alignments=board_alignments,\n",
    "#                                       sequence_length=raw_decoder_lengths,\n",
    "#                                       output_attention=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs: dict):\n",
    "        \n",
    "        player_seed = inputs['player_seed']                 # tf.int32 - (b,)\n",
    "        temperature = inputs['temperature']                  # tf,flt32 - (b,)\n",
    "        dropout_rates = inputs['dropout_rate']               # tf.flt32 - (b,)\n",
    "        # stop_gradient_all = pholder('stop_gradient_all')\n",
    "        board_alignments = inputs['board_alignments']\n",
    "        decoder_inputs = inputs['decoder_inputs']\n",
    "        # decoder_type = inputs['decoder_type']\n",
    "        raw_decoder_lengths = inputs['raw_decoder_lengths']\n",
    "        decoder_lengths = inputs['decoder_lengths']\n",
    "        board_state_conv = inputs['board_state_conv']\n",
    "        order_embedding = inputs['order_embedding']\n",
    "        candidate_embedding = inputs['candidate_embedding']\n",
    "        candidates = inputs['candidates']\n",
    "        max_candidate_length = inputs['max_candidate_length']\n",
    "        \n",
    "        \n",
    "        # ======== Regular Decoding ========\n",
    "        \n",
    "        \n",
    "        something = self.another_decoder_cell(\n",
    "            memory = board_state_conv,\n",
    "            alignments = board_alignments,\n",
    "            memory_sequence_lengths = raw_decoder_lengths\n",
    "        )\n",
    "        \n",
    "        # Applying Dropout to input, attention and output\n",
    "        # decoder_cell = SeededDropoutWrapper(cell=lstm_cell,\n",
    "        #                                     seeds=player_seeds,\n",
    "        #                                     input_keep_probs=1. - dropout_rates,\n",
    "        #                                     output_keep_probs=1. - dropout_rates,\n",
    "        #                                     variational_recurrent=hps('use_v_dropout'),\n",
    "        #                                     input_size=hps('order_emb_size') + hps('attn_size'),\n",
    "        #                                     dtype=tf.float32)\n",
    "        \n",
    "        # something = self.decoder_cell(\n",
    "        #     seeds = player_seed,\n",
    "        #     input_keep_probs = 1. - dropout_rates,\n",
    "        #     output_keep_probs = 1. - dropout_rates,\n",
    "        #     state_keep_probs = torch.tensor([1.]),\n",
    "        # )\n",
    "        \n",
    "        # something = self.\n",
    "        \n",
    "        # print(decoder_cell._input_keep_probs, decoder_cell._input_keep_probs.shape)\n",
    "        # print(decoder_cell._state_keep_probs, decoder_cell._state_keep_probs.shape)\n",
    "        # print(decoder_cell._output_keep_probs, decoder_cell._output_keep_probs.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3df667-3195-43e9-8bae-99f29aa854a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(data_features = DATA_FEATURES, h_params = H_PARAMETERS)\n",
    "decoder = Decoder(data_features = DATA_FEATURES, h_params = H_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2d5a6d3-17e4-47a5-b043-54861680e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utility_functions import seeded_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "6abd49fb-c026-4739-b8cf-4bd628eabe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([128, 200])\n",
      "torch.Size([128, 120])\n",
      "torch.Size([128, 81])\n",
      "False\n",
      "torch.Size([128, 200])\n",
      "torch.Size([128, 120])\n",
      "torch.Size([128, 81])\n"
     ]
    }
   ],
   "source": [
    "first_attempt = None\n",
    "i = 0\n",
    "for batch in dataloader:\n",
    "    # print(batch)\n",
    "    # print(batch[\"request_id\"])\n",
    "    # print(batch[\"player_seed\"], batch[\"player_seed\"].shape)\n",
    "    # print(batch[\"board_state\"], batch[\"board_state\"].shape)\n",
    "    # print(batch[\"board_alignments\"], batch[\"board_alignments\"].shape) ## NEED TO DISABLE FLATTENING\n",
    "    # print(batch[\"prev_orders_state\"], batch[\"prev_orders_state\"].shape)\n",
    "    # print(batch[\"decoder_inputs\"], batch[\"decoder_inputs\"].shape)\n",
    "    # print(batch[\"decoder_lengths\"], batch[\"decoder_lengths\"].shape)\n",
    "    # print(batch[\"candidates\"], batch[\"candidates\"].shape)\n",
    "    # print(batch[\"noise\"], batch[\"noise\"].shape)\n",
    "    # print(batch[\"temperature\"], batch[\"temperature\"].shape, batch[\"temperature\"].dtype)\n",
    "    # print(batch[\"dropout_rate\"], batch[\"dropout_rate\"].shape, batch[\"dropout_rate\"].dtype)\n",
    "    # print(batch[\"current_power\"], batch[\"current_power\"].shape)\n",
    "    # print(batch[\"current_season\"], batch[\"current_season\"].shape)\n",
    "    # print(batch[\"draw_target\"], batch[\"draw_target\"].shape)\n",
    "    # print(batch[\"value_target\"], batch[\"value_target\"].shape)\n",
    "    result = encoder(batch)\n",
    "    first_attempt = decoder(result)\n",
    "    # real_batch = batch\n",
    "    # first_batch = batch\n",
    "    \n",
    "    if i == 1:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd99f1c-ee85-4aca-b345-1588d0b5e8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
